{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Machine Learning Engineer - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified code after successful submission.\n",
    "Refine code and improve UX when entering information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Marco Xu, Dec 19th 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "#pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>cast</th>\n",
       "      <th>crew</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[{'id': 313576, 'name': 'Hot Tub Time Machine ...</td>\n",
       "      <td>14000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt2637294</td>\n",
       "      <td>en</td>\n",
       "      <td>Hot Tub Time Machine 2</td>\n",
       "      <td>When Lou, who has become the \"father of the In...</td>\n",
       "      <td>6.575393</td>\n",
       "      <td>/tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg</td>\n",
       "      <td>[{'name': 'Paramount Pictures', 'id': 4}, {'na...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>2/20/15</td>\n",
       "      <td>93.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>The Laws of Space and Time are About to be Vio...</td>\n",
       "      <td>Hot Tub Time Machine 2</td>\n",
       "      <td>[{'id': 4379, 'name': 'time travel'}, {'id': 9...</td>\n",
       "      <td>[{'cast_id': 4, 'character': 'Lou', 'credit_id...</td>\n",
       "      <td>[{'credit_id': '59ac067c92514107af02c8c8', 'de...</td>\n",
       "      <td>12314651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[{'id': 107674, 'name': 'The Princess Diaries ...</td>\n",
       "      <td>40000000</td>\n",
       "      <td>[{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0368933</td>\n",
       "      <td>en</td>\n",
       "      <td>The Princess Diaries 2: Royal Engagement</td>\n",
       "      <td>Mia Thermopolis is now a college graduate and ...</td>\n",
       "      <td>8.248895</td>\n",
       "      <td>/w9Z7A0GHEhIp7etpj0vyKOeU1Wx.jpg</td>\n",
       "      <td>[{'name': 'Walt Disney Pictures', 'id': 2}]</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>8/6/04</td>\n",
       "      <td>113.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>It can take a lifetime to find true love; she'...</td>\n",
       "      <td>The Princess Diaries 2: Royal Engagement</td>\n",
       "      <td>[{'id': 2505, 'name': 'coronation'}, {'id': 42...</td>\n",
       "      <td>[{'cast_id': 1, 'character': 'Mia Thermopolis'...</td>\n",
       "      <td>[{'credit_id': '52fe43fe9251416c7502563d', 'de...</td>\n",
       "      <td>95149435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3300000</td>\n",
       "      <td>[{'id': 18, 'name': 'Drama'}]</td>\n",
       "      <td>http://sonyclassics.com/whiplash/</td>\n",
       "      <td>tt2582802</td>\n",
       "      <td>en</td>\n",
       "      <td>Whiplash</td>\n",
       "      <td>Under the direction of a ruthless instructor, ...</td>\n",
       "      <td>64.299990</td>\n",
       "      <td>/lIv1QinFqz4dlp5U4lQ6HaiskOZ.jpg</td>\n",
       "      <td>[{'name': 'Bold Films', 'id': 2266}, {'name': ...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>10/10/14</td>\n",
       "      <td>105.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>The road to greatness can take you to the edge.</td>\n",
       "      <td>Whiplash</td>\n",
       "      <td>[{'id': 1416, 'name': 'jazz'}, {'id': 1523, 'n...</td>\n",
       "      <td>[{'cast_id': 5, 'character': 'Andrew Neimann',...</td>\n",
       "      <td>[{'credit_id': '54d5356ec3a3683ba0000039', 'de...</td>\n",
       "      <td>13092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1200000</td>\n",
       "      <td>[{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...</td>\n",
       "      <td>http://kahaanithefilm.com/</td>\n",
       "      <td>tt1821480</td>\n",
       "      <td>hi</td>\n",
       "      <td>Kahaani</td>\n",
       "      <td>Vidya Bagchi (Vidya Balan) arrives in Kolkata ...</td>\n",
       "      <td>3.174936</td>\n",
       "      <td>/aTXRaPrWSinhcmCrcfJK17urp3F.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'iso_3166_1': 'IN', 'name': 'India'}]</td>\n",
       "      <td>3/9/12</td>\n",
       "      <td>122.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kahaani</td>\n",
       "      <td>[{'id': 10092, 'name': 'mystery'}, {'id': 1054...</td>\n",
       "      <td>[{'cast_id': 1, 'character': 'Vidya Bagchi', '...</td>\n",
       "      <td>[{'credit_id': '52fe48779251416c9108d6eb', 'de...</td>\n",
       "      <td>16000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt1380152</td>\n",
       "      <td>ko</td>\n",
       "      <td>마린보이</td>\n",
       "      <td>Marine Boy is the story of a former national s...</td>\n",
       "      <td>1.148070</td>\n",
       "      <td>/m22s7zvkVFDU9ir56PiiqIEWFdT.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'iso_3166_1': 'KR', 'name': 'South Korea'}]</td>\n",
       "      <td>2/5/09</td>\n",
       "      <td>118.0</td>\n",
       "      <td>[{'iso_639_1': 'ko', 'name': '한국어/조선말'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marine Boy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'cast_id': 3, 'character': 'Chun-soo', 'cred...</td>\n",
       "      <td>[{'credit_id': '52fe464b9251416c75073b43', 'de...</td>\n",
       "      <td>3923970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                              belongs_to_collection    budget  \\\n",
       "0   1  [{'id': 313576, 'name': 'Hot Tub Time Machine ...  14000000   \n",
       "1   2  [{'id': 107674, 'name': 'The Princess Diaries ...  40000000   \n",
       "2   3                                                NaN   3300000   \n",
       "3   4                                                NaN   1200000   \n",
       "4   5                                                NaN         0   \n",
       "\n",
       "                                              genres  \\\n",
       "0                     [{'id': 35, 'name': 'Comedy'}]   \n",
       "1  [{'id': 35, 'name': 'Comedy'}, {'id': 18, 'nam...   \n",
       "2                      [{'id': 18, 'name': 'Drama'}]   \n",
       "3  [{'id': 53, 'name': 'Thriller'}, {'id': 18, 'n...   \n",
       "4  [{'id': 28, 'name': 'Action'}, {'id': 53, 'nam...   \n",
       "\n",
       "                            homepage    imdb_id original_language  \\\n",
       "0                                NaN  tt2637294                en   \n",
       "1                                NaN  tt0368933                en   \n",
       "2  http://sonyclassics.com/whiplash/  tt2582802                en   \n",
       "3         http://kahaanithefilm.com/  tt1821480                hi   \n",
       "4                                NaN  tt1380152                ko   \n",
       "\n",
       "                             original_title  \\\n",
       "0                    Hot Tub Time Machine 2   \n",
       "1  The Princess Diaries 2: Royal Engagement   \n",
       "2                                  Whiplash   \n",
       "3                                   Kahaani   \n",
       "4                                      마린보이   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "0  When Lou, who has become the \"father of the In...    6.575393   \n",
       "1  Mia Thermopolis is now a college graduate and ...    8.248895   \n",
       "2  Under the direction of a ruthless instructor, ...   64.299990   \n",
       "3  Vidya Bagchi (Vidya Balan) arrives in Kolkata ...    3.174936   \n",
       "4  Marine Boy is the story of a former national s...    1.148070   \n",
       "\n",
       "                        poster_path  \\\n",
       "0  /tQtWuwvMf0hCc2QR2tkolwl7c3c.jpg   \n",
       "1  /w9Z7A0GHEhIp7etpj0vyKOeU1Wx.jpg   \n",
       "2  /lIv1QinFqz4dlp5U4lQ6HaiskOZ.jpg   \n",
       "3  /aTXRaPrWSinhcmCrcfJK17urp3F.jpg   \n",
       "4  /m22s7zvkVFDU9ir56PiiqIEWFdT.jpg   \n",
       "\n",
       "                                production_companies  \\\n",
       "0  [{'name': 'Paramount Pictures', 'id': 4}, {'na...   \n",
       "1        [{'name': 'Walt Disney Pictures', 'id': 2}]   \n",
       "2  [{'name': 'Bold Films', 'id': 2266}, {'name': ...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                production_countries release_date  runtime  \\\n",
       "0  [{'iso_3166_1': 'US', 'name': 'United States o...      2/20/15     93.0   \n",
       "1  [{'iso_3166_1': 'US', 'name': 'United States o...       8/6/04    113.0   \n",
       "2  [{'iso_3166_1': 'US', 'name': 'United States o...     10/10/14    105.0   \n",
       "3            [{'iso_3166_1': 'IN', 'name': 'India'}]       3/9/12    122.0   \n",
       "4      [{'iso_3166_1': 'KR', 'name': 'South Korea'}]       2/5/09    118.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "2           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "3  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "4           [{'iso_639_1': 'ko', 'name': '한국어/조선말'}]  Released   \n",
       "\n",
       "                                             tagline  \\\n",
       "0  The Laws of Space and Time are About to be Vio...   \n",
       "1  It can take a lifetime to find true love; she'...   \n",
       "2    The road to greatness can take you to the edge.   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                      title  \\\n",
       "0                    Hot Tub Time Machine 2   \n",
       "1  The Princess Diaries 2: Royal Engagement   \n",
       "2                                  Whiplash   \n",
       "3                                   Kahaani   \n",
       "4                                Marine Boy   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [{'id': 4379, 'name': 'time travel'}, {'id': 9...   \n",
       "1  [{'id': 2505, 'name': 'coronation'}, {'id': 42...   \n",
       "2  [{'id': 1416, 'name': 'jazz'}, {'id': 1523, 'n...   \n",
       "3  [{'id': 10092, 'name': 'mystery'}, {'id': 1054...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                cast  \\\n",
       "0  [{'cast_id': 4, 'character': 'Lou', 'credit_id...   \n",
       "1  [{'cast_id': 1, 'character': 'Mia Thermopolis'...   \n",
       "2  [{'cast_id': 5, 'character': 'Andrew Neimann',...   \n",
       "3  [{'cast_id': 1, 'character': 'Vidya Bagchi', '...   \n",
       "4  [{'cast_id': 3, 'character': 'Chun-soo', 'cred...   \n",
       "\n",
       "                                                crew   revenue  \n",
       "0  [{'credit_id': '59ac067c92514107af02c8c8', 'de...  12314651  \n",
       "1  [{'credit_id': '52fe43fe9251416c7502563d', 'de...  95149435  \n",
       "2  [{'credit_id': '54d5356ec3a3683ba0000039', 'de...  13092000  \n",
       "3  [{'credit_id': '52fe48779251416c9108d6eb', 'de...  16000000  \n",
       "4  [{'credit_id': '52fe464b9251416c75073b43', 'de...   3923970  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#Fixes dates which are in 20xx\n",
    "\n",
    "def fix_date(x):\n",
    "    year = x.split('/')[2]\n",
    "    if int(year) <= 19:\n",
    "        return x[:-2] + '20' + year\n",
    "    else:\n",
    "        return x[:-2] + '19' + year\n",
    "    \n",
    "df['release_date'] = df['release_date'].apply(lambda x: fix_date(x))\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1921-01-21 00:00:00')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df.release_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-07-20 00:00:00')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df.release_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_date_year'] = pd.DatetimeIndex(df['release_date']).year\n",
    "df['release_date_month'] = pd.DatetimeIndex(df['release_date']).month\n",
    "df['release_date_quarter'] = pd.DatetimeIndex(df['release_date']).quarter\n",
    "df['release_date_weekofyear'] = pd.DatetimeIndex(df['release_date']).weekofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode variables with several values like genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ast import literal_eval\n",
    "def string_to_list(text,key):\n",
    "    pattern = \"\\{(.*?)\\}\"\n",
    "    substring = re.findall(pattern, str(text))\n",
    "    # convert substring to list of dictionaries\n",
    "    genre_list = [literal_eval('{'+i+'}')[key] for i in substring]\n",
    "    \n",
    "    return genre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres_cleaned'] = df['genres'].apply(lambda x: string_to_list(x,'name'))\n",
    "df['spoken_languages_cleaned'] = df['spoken_languages'].apply(lambda x: string_to_list(x,'name'))\n",
    "df['production_countries_cleaned'] = df['production_countries'].apply(lambda x: string_to_list(x,'name'))\n",
    "df['production_companies_cleaned'] = df['production_companies'].apply(lambda x: string_to_list(x,'name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def list_to_columns(df,column):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df_encoded = pd.DataFrame(mlb.fit_transform(df[column]),columns=mlb.classes_, index=df.index)\n",
    "    df = df.join(df_encoded)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = list_to_columns(df,\"genres_cleaned\")\n",
    "df = list_to_columns(df,\"spoken_languages_cleaned\")\n",
    "df = list_to_columns(df,\"production_countries_cleaned\")\n",
    "df = list_to_columns(df,\"production_companies_cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Information from cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get top actors\n",
    "df_top_movies = df.sort_values(by=['revenue'], ascending=False).head(100)\n",
    "all_actors = df_top_movies['cast'].apply(lambda x: string_to_list(x,'name')).to_list()\n",
    "top_actors = [actor[:2] for actor in all_actors]\n",
    "\n",
    "top_actors_distinct = []\n",
    "for sublist in top_actors:\n",
    "    for actor in sublist:\n",
    "        if actor not in top_actors_distinct:\n",
    "            top_actors_distinct.append(actor)\n",
    "            \n",
    "top_actors_distinct\n",
    "## note: we save this list, since in test data, we don't have revenue information\n",
    "top_actors_distinct_df = pd.DataFrame(top_actors_distinct,columns=['actor'])\n",
    "top_actors_distinct_df.to_csv('top_actors_distinct_df.csv', index=False)\n",
    "top_actors_distinct = pd.read_csv('top_actors_distinct_df.csv').actor.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get main_cast\n",
    "cast_list = df['cast'].apply(lambda x: string_to_list(x,'name')).to_list()\n",
    "main_cast = [actor[:3] for actor in cast_list]\n",
    "df['main_cast'] = main_cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast in top_actors included?\n",
    "def cast_in_top_actors(cast,top_actors_list):\n",
    "    if not list(set(cast) & set(top_actors_list)): #check if intersection is empty\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "df['top_actor_included'] = df['main_cast'].apply(lambda x: cast_in_top_actors(x,top_actors_distinct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how much did main cast already gross?\n",
    "df_cast_gross = df[['revenue','main_cast']]\n",
    "df_cast_gross = list_to_columns(df_cast_gross,\"main_cast\")\n",
    "main_cast_list = df_cast_gross.columns.tolist()[2:]\n",
    "\n",
    "actor_revenue = {}\n",
    "for actor in main_cast_list:\n",
    "    df_actor_revenue = df_cast_gross[['revenue',actor]]\n",
    "    df_actor_revenue = df_actor_revenue.groupby([actor]).sum().reset_index()\n",
    "    revenue = df_actor_revenue[df_actor_revenue[actor]==1].reset_index().revenue[0]\n",
    "    actor_revenue[actor] = revenue\n",
    "    \n",
    "actor_revenue_df = pd.DataFrame.from_dict(actor_revenue, orient='index',columns=['revenue']).reset_index().rename(columns={'index': 'actor'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: we save this df, since in test data, we don't have revenue information\n",
    "actor_revenue_df.to_csv('actor_revenue_df.csv', index=False)\n",
    "actor_revenue_df = pd.read_csv('actor_revenue_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_revenue_group = {}\n",
    "i = 1\n",
    "for cast in df.main_cast.tolist():\n",
    "    sum_revenue = actor_revenue_df[actor_revenue_df.actor.isin(cast)].revenue.sum()\n",
    "    actor_revenue_group[i] = sum_revenue\n",
    "    i += 1\n",
    "    \n",
    "actor_revenue_group_df = pd.DataFrame.from_dict(actor_revenue_group, orient='index',columns=['revenue']).reset_index().rename(columns={'index': 'id','revenue': 'revenue_cast'})\n",
    "actor_revenue_group_df = actor_revenue_group_df[['revenue_cast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(actor_revenue_group_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Information from franchise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check if film is part of franchise\n",
    "df['part_of_franchise'] = np.where(df['belongs_to_collection'].isnull(), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much did each franchise earn?\n",
    "franchise_list = df['belongs_to_collection'].apply(lambda x: string_to_list(x,'name')).to_list()\n",
    "franchise = [np.nan if not franchise else franchise[0] for franchise in franchise_list]\n",
    "df['franchise'] = franchise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_franchise_revenue = df.groupby([\"franchise\"]).sum()[['revenue']].reset_index()\n",
    "df_franchise_revenue = df_franchise_revenue.rename(columns={'revenue': 'franchise_revenue'})\n",
    "\n",
    "## note: we save this df, since in test data, we don't have revenue information\n",
    "df_franchise_revenue.to_csv('df_franchise_revenue.csv', index=False)\n",
    "df_franchise_revenue = pd.read_csv('df_franchise_revenue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_franchise_revenue, on = 'franchise', how = 'left')\n",
    "df['franchise_revenue'] = df['franchise_revenue'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## is production company part of \"big five\"? https://en.wikipedia.org/wiki/Major_film_studios\n",
    "major_film_studios_list = ['Universal Pictures','Paramount Pictures', 'Warner Bros. Pictures', 'New Line Cinema', \n",
    "                           'Walt Disney Pictures', '20th Century Studios', 'Columbia Pictures', 'TriStar Pictures']\n",
    "\n",
    "def is_big_five(productin_company, major_film_studios_list):\n",
    "    if not list(set(productin_company) & set(major_film_studios_list)): #check if intersection is empty\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "df['is_big_five'] = df['production_companies_cleaned'].apply(lambda x: is_big_five(x,major_film_studios_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['budget_runtime_ratio'] = (df['budget']/df['runtime']).fillna(0)\n",
    "df['budget_year_ratio'] = (df['budget']/(df['release_date_year']*df['release_date_year'])).fillna(0)\n",
    "df['franchise_buget_ratio'] = (df['franchise_revenue']/df['budget']).fillna(0) \n",
    "df['cast_buget_ratio'] = (df['revenue_cast']/df['budget']).fillna(0) \n",
    "\n",
    "df = df.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust for Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inflationBudget'] = df['budget'] + df['budget']*1.8/100*(2018-df['release_date_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['budget_log'] = np.log1p(df['budget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.drop(columns=['id', 'belongs_to_collection','genres','homepage','imdb_id','original_title','overview','popularity',\n",
    "                    'poster_path','production_companies','production_countries','release_date','spoken_languages','status',\n",
    "                    'tagline','title','Keywords','cast','crew','genres_cleaned','spoken_languages_cleaned','production_countries_cleaned',\n",
    "                            'main_cast','franchise','production_companies_cleaned','original_language'])\n",
    "\n",
    "\n",
    "df_small.dropna(inplace=True) #remove rows including na\n",
    "df_small = df_small.drop(df_small[df_small.budget <= 0].index) #drop rows with zero budget. it's not possible to have movies with zero budget\n",
    "\n",
    "np.random.seed(23)\n",
    "df_small = df_small.sample(frac = 1) \n",
    "\n",
    "X = df_small.drop(columns=['revenue'])\n",
    "y = df_small[['revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2187, 3862)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set has many features due to encoding. However, many of them appear few times, so they have almost no variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove features with very little variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "threshold = .99 # the higher, the more features\n",
    "sel = VarianceThreshold(threshold=(threshold * (1 - threshold)))\n",
    "sel.fit_transform(X)\n",
    "X = X.iloc[:,sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2187, 86)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After features with almost no variance have been removed, 86 are left. This is a good number to start modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.05, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save this file: necessary in inference\n",
    "\n",
    "X_train.head().to_csv('X_head.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "X_train['revenue'] = y_train\n",
    "X_test['revenue'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('movies_train.csv', index=False)\n",
    "X_test.to_csv('movies_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# S3 prefix (which folder will we use)\n",
    "prefix = 'box_office_prediction_linear_regression'\n",
    "\n",
    "\n",
    "# send data to S3. SageMaker will take training data from s3\n",
    "trainpath = session.upload_data(\n",
    "    path='movies_train.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')\n",
    "\n",
    "testpath = session.upload_data(\n",
    "    path='movies_test.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_revenue_df = pd.read_csv('actor_revenue_df.csv')\n",
    "top_actors_distinct = pd.read_csv('top_actors_distinct_df.csv').actor.to_list()\n",
    "df_franchise_revenue = pd.read_csv('df_franchise_revenue.csv')\n",
    "X = pd.read_csv('X_head.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##upload mapping tables to s3\n",
    "# send data to S3. SageMaker will take training data from s3\n",
    "actor_revenue_df_path = session.upload_data(\n",
    "    path='actor_revenue_df.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')\n",
    "\n",
    "top_actors_distinct_path = session.upload_data(\n",
    "    path='top_actors_distinct_df.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')\n",
    "\n",
    "df_franchise_revenue_path = session.upload_data(\n",
    "    path='df_franchise_revenue.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')\n",
    "\n",
    "X_path = session.upload_data(\n",
    "    path='X_head.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/sklearncontainer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a simple linear regression serves as base model. The algorithm is simple and serves as benchmark model in many cases.\n",
    "Second, a random forest regressor is used for training and prediction. Random forests often perform very well and can handle training sets with many inputs. Also it can show feature importance which might be interesting for film producers.\n",
    "Therefore, it's perfect for our use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_predict_random_forest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_predict_random_forest.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = joblib.load(os.path.join(model_dir, \"rf_model.joblib\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    print(input_data)\n",
    "    return model.predict(input_data)\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument('--n-estimators', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='movies_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='movies_test.csv')\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print('building training and testing datasets')\n",
    "    X_train = train_df.drop(columns=[args.target])\n",
    "    X_test = test_df.drop(columns=[args.target])\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # train\n",
    "    print('training model')\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators,\n",
    "        min_samples_leaf=args.min_samples_leaf,\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f'R2 on training set: {model.score(X_train, y_train)}')\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    print(f'R2 on test set: {r2_score(y_test,predictions)}')\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"rf_model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print('model persisted at ' + path)\n",
    "    \n",
    "    \n",
    "    # feature importance\n",
    "    feature_imp = pd.DataFrame(data= {'feature': X_train.columns, 'importance':model.feature_importances_})\n",
    "    print(feature_imp.sort_values(by=['importance'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting arguments\n",
      "reading data\n",
      "building training and testing datasets\n",
      "training model\n",
      "R2 on training set: 0.961069622105539\n",
      "R2 on test set: 0.7777330947351262\n",
      "model persisted at ./rf_model.joblib\n",
      "                      feature  importance\n",
      "78          franchise_revenue    0.320513\n",
      "81          budget_year_ratio    0.216113\n",
      "76               revenue_cast    0.083721\n",
      "84            inflationBudget    0.058505\n",
      "0                      budget    0.044134\n",
      "..                        ...         ...\n",
      "37                    한국어/조선말    0.000087\n",
      "59  Metro-Goldwyn-Mayer (MGM)    0.000076\n",
      "47                     Russia    0.000055\n",
      "33                     हिन्दी    0.000014\n",
      "34                      தமிழ்    0.000007\n",
      "\n",
      "[86 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "! python train_predict_random_forest.py --n-estimators 50 \\\n",
    "                   --min-samples-leaf 1 \\\n",
    "                   --model-dir ./ \\\n",
    "                   --train ./ \\\n",
    "                   --test ./ \\\n",
    "                   --target revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs better than the benchmark model Linear Regression (higher R-squared on both training and testing set). Also, franchise reveneue, budget and revenue_cast seem to be important features, which is not surprising.\n",
    "\n",
    "Model performs very well on training but worse on testing set, which is an indicator for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Random Forest Model with inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting serve/deploy_random_forest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile serve/deploy_random_forest.py\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import ast\n",
    "\n",
    "from utils import select_replace_columns, cast_in_top_actors, list_to_columns, is_big_five\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    if content_type == 'text/plain':\n",
    "    \n",
    "        movie_dict = ast.literal_eval(request_body.decode('utf-8'))\n",
    "        print('data imported')\n",
    "        df = pd.DataFrame.from_dict(movie_dict)\n",
    "        \n",
    "        actor_revenue_df = pd.read_csv('s3://sagemaker-eu-central-1-724293319711/sagemaker/sklearncontainer/actor_revenue_df.csv')\n",
    "        top_actors_distinct = pd.read_csv('s3://sagemaker-eu-central-1-724293319711/sagemaker/sklearncontainer/top_actors_distinct_df.csv')\n",
    "        df_franchise_revenue = pd.read_csv('s3://sagemaker-eu-central-1-724293319711/sagemaker/sklearncontainer/df_franchise_revenue.csv')\n",
    "        X = pd.read_csv('s3://sagemaker-eu-central-1-724293319711/sagemaker/sklearncontainer/X_head.csv')\n",
    "\n",
    "        major_film_studios_list = ['Universal Pictures','Paramount Pictures', 'Warner Bros. Pictures', 'New Line Cinema', 'Walt Disney Pictures',\n",
    "                                  '20th Century Studios', 'Columbia Pictures', 'TriStar Pictures']\n",
    "\n",
    "\n",
    "        print('mapping tables loaded')\n",
    "\n",
    "\n",
    "        df['release_date_year'] = pd.DatetimeIndex(df['release_date']).year\n",
    "        df['release_date_month'] = pd.DatetimeIndex(df['release_date']).month\n",
    "        df['release_date_quarter'] = pd.DatetimeIndex(df['release_date']).weekofyear\n",
    "        df['release_date_weekofyear'] = pd.DatetimeIndex(df['release_date']).weekofyear\n",
    "\n",
    "\n",
    "        df = list_to_columns(df,\"genres\")\n",
    "        df = list_to_columns(df,\"spoken_languages\")\n",
    "        df = list_to_columns(df,\"production_countries\")\n",
    "        df = list_to_columns(df,\"production_companies\")\n",
    "\n",
    "\n",
    "        actor_revenue_group = {}\n",
    "        i = 1\n",
    "        for cast in df.main_cast.tolist():\n",
    "            sum_revenue = actor_revenue_df[actor_revenue_df.actor.isin(cast)].revenue.sum()\n",
    "            actor_revenue_group[i] = sum_revenue\n",
    "            i += 1\n",
    "\n",
    "        actor_revenue_group_df = pd.DataFrame.from_dict(actor_revenue_group, orient='index',columns=['revenue']).reset_index().rename(columns={'index': 'id','revenue': 'revenue_cast'})\n",
    "        actor_revenue_group_df = actor_revenue_group_df[['revenue_cast']]\n",
    "        df = df.join(actor_revenue_group_df)\n",
    "        print('cast revenue')\n",
    "\n",
    "        df['top_actor_included'] = df['main_cast'].apply(lambda x: cast_in_top_actors(x,top_actors_distinct))\n",
    "\n",
    "        df['part_of_franchise'] = np.where(df['franchise'].isnull(), 0, 1)\n",
    "\n",
    "        df = df.merge(df_franchise_revenue, on = 'franchise', how = 'left')\n",
    "        df['franchise_revenue'] = df['franchise_revenue'].fillna(0)\n",
    "        print('franchise')\n",
    "\n",
    "        df['is_big_five'] = df['production_companies'].apply(lambda x: is_big_five(x,major_film_studios_list))\n",
    "\n",
    "        df['budget_runtime_ratio'] = (df['budget']/df['runtime']).fillna(0)\n",
    "        df['budget_year_ratio'] = (df['budget']/(df['release_date_year']*df['release_date_year'])).fillna(0)\n",
    "        df['franchise_buget_ratio'] = (df['franchise_revenue']/df['budget']).fillna(0) \n",
    "        df['cast_buget_ratio'] = (df['revenue_cast']/df['budget']).fillna(0) \n",
    "        print('ratios')\n",
    "\n",
    "        df = df.replace([np.inf, -np.inf], 0)\n",
    "        df['inflationBudget'] = df['budget'] + df['budget']*1.8/100*(2018-df['release_date_year'])\n",
    "        df['budget_log'] = np.log1p(df['budget'])\n",
    "\n",
    "        df = select_replace_columns(df,X)\n",
    "\n",
    "\n",
    "        df_array = np.array(df)\n",
    "        print(df_array )\n",
    "        return df_array \n",
    "    else:\n",
    "        print('wrong content_type')\n",
    "\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    print(input_data)\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument('--n-estimators', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=3)\n",
    "    parser.add_argument('--bootstrap', type=bool, default=True)\n",
    "    parser.add_argument('--max_depth', type=int, default=100)\n",
    "    parser.add_argument('--min_samples_split', type=int, default=2)\n",
    "   \n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "    parser.add_argument('--train-file', type=str, default='movies_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='movies_test.csv')\n",
    "    parser.add_argument('--target', type=str) # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    print('building training and testing datasets')\n",
    "    X_train = train_df.drop(columns=[args.target])\n",
    "    X_test = test_df.drop(columns=[args.target])\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # train\n",
    "    print('training model')\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators,\n",
    "        min_samples_leaf=args.min_samples_leaf,\n",
    "        max_depth=args.max_depth,\n",
    "        bootstrap=args.bootstrap,\n",
    "        min_samples_split=args.min_samples_split,\n",
    "        n_jobs=-1)\n",
    "    #print(X_train)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print('model persisted at ' + path)\n",
    "    print(args.min_samples_leaf)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# We use the Estimator from the SageMaker Python SDK\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "FRAMEWORK_VERSION = '0.23-1'\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point='deploy_random_forest.py',\n",
    "    source_dir='serve',\n",
    "    role = get_execution_role(),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.xlarge',\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name='rf-deployed',\n",
    "    metric_definitions=[\n",
    "        {'Name': 'median-AE',\n",
    "         'Regex': \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n",
    "    hyperparameters = {'n-estimators': 80,\n",
    "                       'min-samples-leaf': 3,\n",
    "                       'max_depth': 200,\n",
    "                       'bootstrap': True,\n",
    "                       'min_samples_split': 6,\n",
    "                       'target': 'revenue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-19 16:13:21 Starting - Starting the training job...\n",
      "2020-12-19 16:13:45 Starting - Launching requested ML instancesProfilerReport-1608394400: InProgress\n",
      "......\n",
      "2020-12-19 16:14:46 Starting - Preparing the instances for training...\n",
      "2020-12-19 16:15:21 Downloading - Downloading input data...\n",
      "2020-12-19 16:15:47 Training - Downloading the training image..\u001b[34m2020-12-19 16:16:01,170 sagemaker-training-toolkit INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:01,172 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:01,182 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:01,532 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-0.5.2-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-0.8.5-py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.1.2-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34mCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.7.1-py3-none-any.whl (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.17.45,>=1.17.44\n",
      "  Downloading botocore-1.17.44-py2.py3-none-any.whl (6.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp>=3.3.1\n",
      "  Downloading aiohttp-3.7.3-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mCollecting typing_extensions>=3.7\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mCollecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /miniconda3/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs->-r requirements.txt (line 1)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=17.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<4.0,>=2.0 in /miniconda3/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0 in /miniconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->-r requirements.txt (line 1)) (2.10)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=70948 sha256=7546da86a33b99221a2103577c3bfd88e30e4c7d2931bcbeb6ad59a3c8fe5ebc\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\u001b[0m\n",
      "\u001b[34mSuccessfully built wrapt\u001b[0m\n",
      "\u001b[34mInstalling collected packages: fsspec, typing-extensions, aioitertools, wrapt, docutils, botocore, attrs, multidict, yarl, async-timeout, aiohttp, aiobotocore, s3fs\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.4\n",
      "    Uninstalling botocore-1.19.4:\n",
      "      Successfully uninstalled botocore-1.19.4\u001b[0m\n",
      "\u001b[34mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\u001b[0m\n",
      "\u001b[34mWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\u001b[0m\n",
      "\u001b[34mboto3 1.16.4 requires botocore<1.20.0,>=1.19.4, but you'll have botocore 1.17.44 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiobotocore-1.1.2 aiohttp-3.7.3 aioitertools-0.7.1 async-timeout-3.0.1 attrs-20.3.0 botocore-1.17.44 docutils-0.15.2 fsspec-0.8.5 multidict-5.1.0 s3fs-0.5.2 typing-extensions-3.7.4.3 wrapt-1.12.1 yarl-1.6.3\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:08,093 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:08,107 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:08,118 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:08,128 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_depth\": 200,\n",
      "        \"min_samples_split\": 6,\n",
      "        \"n-estimators\": 80,\n",
      "        \"min-samples-leaf\": 3,\n",
      "        \"bootstrap\": true,\n",
      "        \"target\": \"revenue\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"rf-deployed-2020-12-19-16-13-20-781\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-724293319711/rf-deployed-2020-12-19-16-13-20-781/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"deploy_random_forest\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"deploy_random_forest.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"bootstrap\":true,\"max_depth\":200,\"min-samples-leaf\":3,\"min_samples_split\":6,\"n-estimators\":80,\"target\":\"revenue\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=deploy_random_forest.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=deploy_random_forest\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-724293319711/rf-deployed-2020-12-19-16-13-20-781/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"bootstrap\":true,\"max_depth\":200,\"min-samples-leaf\":3,\"min_samples_split\":6,\"n-estimators\":80,\"target\":\"revenue\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"rf-deployed-2020-12-19-16-13-20-781\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-724293319711/rf-deployed-2020-12-19-16-13-20-781/source/sourcedir.tar.gz\",\"module_name\":\"deploy_random_forest\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"deploy_random_forest.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--bootstrap\",\"True\",\"--max_depth\",\"200\",\"--min-samples-leaf\",\"3\",\"--min_samples_split\",\"6\",\"--n-estimators\",\"80\",\"--target\",\"revenue\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=200\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_SAMPLES_SPLIT=6\u001b[0m\n",
      "\u001b[34mSM_HP_N-ESTIMATORS=80\u001b[0m\n",
      "\u001b[34mSM_HP_MIN-SAMPLES-LEAF=3\u001b[0m\n",
      "\u001b[34mSM_HP_BOOTSTRAP=true\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET=revenue\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python deploy_random_forest.py --bootstrap True --max_depth 200 --min-samples-leaf 3 --min_samples_split 6 --n-estimators 80 --target revenue\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2020-12-19 16:16:19 Uploading - Uploading generated training model\n",
      "2020-12-19 16:16:19 Completed - Training job completed\n",
      "\u001b[34mextracting arguments\u001b[0m\n",
      "\u001b[34mreading data\u001b[0m\n",
      "\u001b[34mbuilding training and testing datasets\u001b[0m\n",
      "\u001b[34mtraining model\u001b[0m\n",
      "\u001b[34mmodel persisted at /opt/ml/model/model.joblib\u001b[0m\n",
      "\u001b[34m3\u001b[0m\n",
      "\u001b[34m2020-12-19 16:16:10,451 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 58\n",
      "Billable seconds: 58\n"
     ]
    }
   ],
   "source": [
    "sklearn_estimator.fit({'train':trainpath, 'test': testpath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn_estimator.deploy(instance_type='ml.t2.medium',\n",
    "                                     initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf-deployed-2020-12-19-16-16-32-676\n"
     ]
    }
   ],
   "source": [
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# create sagemaker client using boto3\n",
    "client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dict = \"{'budget':10000,'main_cast': [['Leonardo DiCaprio','Tom Hardy']],'runtime':120,'release_date':'2019/03/23','genres': [['Action','Crime']],'franchise':'The Avengers Collection','spoken_languages': [['en','jp']],'production_countries': [['United States of America','South Korea']],'production_companies': [['Walt Disney Pictures','Paramount Pictures']]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[221289265.754412]'\n"
     ]
    }
   ],
   "source": [
    "#test_data = \"300,10,3\"\n",
    "\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint,\n",
    "    ContentType=\"text/plain\",\n",
    "    Body=test_data_dict\n",
    "    )\n",
    "\n",
    "print(response['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
